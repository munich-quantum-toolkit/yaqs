# Copyright (c) 2025 - 2026 Chair for Design Automation, TUM
# All rights reserved.
#
# SPDX-License-Identifier: MIT
#
# Licensed under the MIT License

"""Fast Matrix Exponential Methods.

This module implements matrix-free methods for approximating the action of a matrix exponential
on a vector via Krylov subspace techniques. It provides an implementation of the Lanczos iteration
to generate an orthonormal basis for the Krylov subspace, and uses this basis to compute an
approximation of exp(-1j * dt * A) * v without explicitly constructing the matrix A.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import scipy.linalg

if TYPE_CHECKING:
    from collections.abc import Callable

    from numpy.typing import NDArray



def lanczos_iteration(
    matrix_free_operator: Callable[[NDArray[np.complex128]], NDArray[np.complex128]],
    vec: NDArray[np.complex128],
    lanczos_iterations: int,
) -> tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.complex128]]:
    """Perform a matrix-free Lanczos iteration.

    This function generates an orthonormal basis for the Krylov subspace of the operator defined
    by matrix_free_operator using the Lanczos algorithm. It computes the diagonal (alpha) and off-diagonal (beta)
    elements of the tridiagonal (Hessenberg) matrix associated with the iteration and returns the
    matrix whose columns are the Lanczos vectors.

    Args:
        matrix_free_operator (Callable[[NDArray[np.complex128]], NDArray[np.complex128]]):
            A function that applies a linear transformation to a vector without explicitly constructing
            the matrix (i.e., matrix-free).
        vec (NDArray[np.complex128]):
            The starting vector for the Lanczos iteration. This vector is normalized in place.
        lanczos_iterations (int):
            The number of Lanczos iterations to perform. It should be much smaller than the dimension of
            vec.

    Returns:
        tuple:
            - alpha (NDArray[np.float64]): Array of length lanczos_iterations containing the diagonal entries of the
              tridiagonal matrix.
            - beta (NDArray[np.float64]): Array of length lanczos_iterations-1 containing the off-diagonal entries.
            - lanczos_mat (NDArray[np.complex128]): A matrix of shape (len(vec) x lanczos_iterations) whose
              columns are the orthonormal Lanczos vectors.

    Raises:
        ValueError: If the starting vector has zero norm.
    """
    v0 = np.array(vec, dtype=np.complex128, copy=True)
    nrm = np.linalg.norm(v0)
    if nrm == 0:
        msg = "Starting vector has zero norm."
        raise ValueError(msg)
    v0 /= nrm

    m = lanczos_iterations
    alpha = np.zeros(m, dtype=np.float64)
    beta = np.zeros(m - 1, dtype=np.float64)

    # Store basis as (n, m): V[:, j] is contiguous and BLAS-friendly
    v = np.zeros((v0.size, m), dtype=np.complex128)
    v[:, 0] = v0

    eps_cut = 100.0 * v0.size * np.finfo(np.float64).eps

    for j in range(m - 1):
        vj = v[:, j]
        w = matrix_free_operator(vj)  # expect shape (n,)

        # alpha_j = <v_j, w>
        aj = np.vdot(vj, w).real
        alpha[j] = aj

        # w <- w - aj*vj - beta_{j-1}*v_{j-1}
        w -= aj * vj
        if j > 0:
            w -= beta[j - 1] * v[:, j - 1]

        bj = np.linalg.norm(w)
        beta[j] = bj

        if bj < eps_cut:
            # Early termination: return truncated basis
            k = j + 1
            return alpha[:k], beta[: k - 1], v[:, :k]

        v[:, j + 1] = w / bj

    # Final alpha_{m-1}
    vj = v[:, m - 1]
    w = matrix_free_operator(vj)
    alpha[m - 1] = np.vdot(vj, w).real
    return alpha, beta, v


    # If we reach here, we exhausted max_lanczos_iterations
    # Check if we can reuse the cached eigendecomposition
    if cached_eigvals is not None and cached_k == m_max:
        # Reuse the cached eigendecomposition from the last error check
        coeffs = vec_norm * np.exp(-1j * dt * cached_eigvals) * cached_eigvecs[0]
        return np.asarray(v @ (cached_eigvecs @ coeffs), dtype=np.complex128)
    
    # Otherwise compute fresh (shouldn't happen often)
    return _compute_krylov_result(alpha, beta, v, vec_norm, dt)


try:
    from .lanczos_numba import normalize_and_store, orthogonalize_step
    HAS_NUMBA = True
except ImportError:
    HAS_NUMBA = False

def expm_krylov(
    matrix_free_operator: Callable[[NDArray[np.complex128]], NDArray[np.complex128]],
    vec: NDArray[np.complex128],
    dt: float,
    max_lanczos_iterations: int = 25,
    tol: float = 1e-12,
) -> NDArray[np.complex128]:
    """Compute the Krylov subspace approximation of the matrix exponential applied to a vector.

    This function approximates exp(-1j * dt * A) * v by projecting the action of the matrix exponential
    onto a Krylov subspace generated by the Lanczos iteration. The method is based on the approach
    described by Hochbruck and Lubich.

    Adaptive Step Size Control:
    The iteration proceeds until the estimated error is below the provided tolerance `tol` or
    `max_lanczos_iterations` is reached. The error is estimated using the residual of the
    Lanczos recurrence relation.

    Args:
        matrix_free_operator (Callable[[NDArray[np.complex128]], NDArray[np.complex128]]):
            A function implementing the matrix-free application of the linear operator A.
        vec (NDArray[np.complex128]):
            The input vector to which the matrix exponential is applied.
        dt (float):
            The time step (or scalar multiplier) in the exponential.
        max_lanczos_iterations (int):
            The maximum number of Lanczos iterations to perform. Default is 25.
        tol (float):
            The convergence tolerance. Default is 1e-12.

    Returns:
        NDArray[np.complex128]:
            The approximate result of applying exp(-1j * dt * A) to vec.
    """
    vec_norm = np.linalg.norm(vec)
    if vec_norm == 0:
        return vec

    # We need to reimplement Lanczos to support adaptive stopping.
    # We will build alpha, beta, and V incrementally.
    m_max = max_lanczos_iterations
    alpha = np.zeros(m_max, dtype=np.float64)
    beta = np.zeros(m_max - 1, dtype=np.float64)
    
    # If using Numba, we prefer Fortran order so that columns v[:, j] are contiguous in memory...
    # BUT for high bond dimensions, interacting with C-ordered w from tensordot might be costly?
    # Let's test C-order always.
    use_numba = HAS_NUMBA and vec.size >= 8192
    
    order = 'C' # Force C-order to test
    v = np.zeros((vec.size, m_max), dtype=np.complex128, order=order)

    v0 = vec / vec_norm
    v[:, 0] = v0

    # Small constant for breakdown check
    eps_cut = 100.0 * vec.size * np.finfo(np.float64).eps

    # Cache for eigendecomposition to avoid recomputing if we reach max iterations
    cached_eigvals = None
    cached_eigvecs = None
    cached_k = None

    for j in range(m_max):
        vj = v[:, j]
        w = matrix_free_operator(vj)

        if use_numba:
            # Use JIT-compiled kernel for orthogonalization
            bj = orthogonalize_step(v, w, j, alpha, beta)
            
            if j < m_max - 1:
                if bj < eps_cut:
                    k = j + 1
                    return _compute_krylov_result(alpha[:k], beta[: k - 1], v[:, :k], vec_norm, dt)
                    
                normalize_and_store(v, w, j, bj)
        else:
            # Pure Python implementation
            # alpha_j = <v_j, w>
            aj = np.vdot(vj, w).real
            alpha[j] = aj

            # Orthogonalize
            w -= aj * vj
            if j > 0:
                w -= beta[j - 1] * v[:, j - 1]

            # Beta_j
            if j < m_max - 1:
                bj = np.linalg.norm(w)
                beta[j] = bj

                if bj < eps_cut:
                    k = j + 1
                    return _compute_krylov_result(alpha[:k], beta[: k - 1], v[:, :k], vec_norm, dt)

                v[:, j + 1] = w / bj

        # Error check (adaptive)
        # We need to compute the exponential of the current T_j
        # Error approx: beta_j * | [exp(-i*dt*T_j)]_{j, 0} |  (last element of first column)
        # We only do this if j >= 1 (at least 2 vectors) to have a meaningful check, or even later.
        if j >= 1:
            # Current tridiagonal matrix T_k of size k x k where k = j + 1
            k = j + 1
            _alpha = alpha[:k]
            _beta = beta[: k - 1]  # beta has length k-1

            # Diagonalize T_k
            try:
                w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
                    _alpha,
                    _beta,
                    lapack_driver="stemr",
                    check_finite=False,
                )
            except scipy.linalg.LinAlgError:
                w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
                    _alpha,
                    _beta,
                    lapack_driver="stebz",
                    check_finite=False,
                )

            # Cache this eigendecomposition
            cached_eigvals = w_hess
            cached_eigvecs = u_hess
            cached_k = k

            coeffs = np.exp(-1j * dt * w_hess) * u_hess[0, :]
            phi_last = np.dot(u_hess[k - 1, :], coeffs)

            if j < m_max - 1:
                # Note: beta array is 0-indexed, so beta[j] corresponds to beta_{j} (the one we just computed)
                # Ensure we have computed it (we did above)
                err = beta[j] * abs(phi_last)
                if err < tol:
                    # Converged
                    y_k = u_hess @ coeffs
                    return np.asarray(v[:, :k] @ (y_k * vec_norm), dtype=np.complex128)

    # If we reach here, we exhausted max_lanczos_iterations
    # Check if we can reuse the cached eigendecomposition
    if cached_eigvals is not None and cached_k == m_max:
        # Reuse the cached eigendecomposition from the last error check
        coeffs = vec_norm * np.exp(-1j * dt * cached_eigvals) * cached_eigvecs[0]
        return np.asarray(v @ (cached_eigvecs @ coeffs), dtype=np.complex128)
    
    # Otherwise compute fresh (shouldn't happen often)
    return _compute_krylov_result(alpha, beta, v, vec_norm, dt)


def _compute_krylov_result(
    alpha: NDArray[np.float64],
    beta: NDArray[np.float64],
    lanczos_mat: NDArray[np.complex128],
    nrm: float,
    dt: float,
) -> NDArray[np.complex128]:
    """Helper to compute final result from T and V."""
    try:
        w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
            alpha,
            beta,
            lapack_driver="stemr",
            check_finite=False,
        )
    except scipy.linalg.LinAlgError:
        w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
            alpha,
            beta,
            lapack_driver="stebz",
            check_finite=False,
        )
    coeffs = nrm * np.exp(-1j * dt * w_hess) * u_hess[0]
    return np.asarray(lanczos_mat @ (u_hess @ coeffs), dtype=np.complex128)

