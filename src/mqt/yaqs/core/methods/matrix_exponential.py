# Copyright (c) 2025 - 2026 Chair for Design Automation, TUM
# All rights reserved.
#
# SPDX-License-Identifier: MIT
#
# Licensed under the MIT License

"""Fast Matrix Exponential Methods.

This module implements matrix-free methods for approximating the action of a matrix exponential
on a vector via Krylov subspace techniques. It provides an implementation of the Lanczos iteration
to generate an orthonormal basis for the Krylov subspace, and uses this basis to compute an
approximation of exp(-1j * dt * A) * v without explicitly constructing the matrix A.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import scipy.linalg

if TYPE_CHECKING:
    from collections.abc import Callable

    from numpy.typing import NDArray

try:
    from .lanczos_numba import normalize_and_store, orthogonalize_step

    HAS_NUMBA = True
except ImportError:
    HAS_NUMBA = False


def expm_krylov(
    matrix_free_operator: Callable[[NDArray[np.complex128]], NDArray[np.complex128]],
    vec: NDArray[np.complex128],
    dt: float,
    max_lanczos_iterations: int = 25,
    tol: float = 1e-12,
) -> NDArray[np.complex128]:
    """Compute the Krylov subspace approximation of the matrix exponential applied to a vector.

    This function approximates exp(-1j * dt * A) * v by projecting the action of the matrix exponential
    onto a Krylov subspace generated by the Lanczos iteration. The method is based on the approach
    described by Hochbruck and Lubich.

    Adaptive Step Size Control:
    The iteration proceeds until the estimated error is below the provided tolerance `tol` or
    `max_lanczos_iterations` is reached. The error is estimated using the residual of the
    Lanczos recurrence relation.

    Args:
        matrix_free_operator (Callable[[NDArray[np.complex128]], NDArray[np.complex128]]):
            A function implementing the matrix-free application of the linear operator A.
        vec (NDArray[np.complex128]):
            The input vector to which the matrix exponential is applied.
        dt (float):
            The time step (or scalar multiplier) in the exponential.
        max_lanczos_iterations (int):
            The maximum number of Lanczos iterations to perform. Default is 25.
        tol (float):
            The convergence tolerance. Default is 1e-12.

    Returns:
        NDArray[np.complex128]:
            The approximate result of applying exp(-1j * dt * A) to vec.
    """
    vec_norm = np.linalg.norm(vec)
    if vec_norm == 0:
        return vec

    # We need to reimplement Lanczos to support adaptive stopping.
    # We will build alpha, beta, and V incrementally.
    m_max = max_lanczos_iterations
    alpha = np.zeros(m_max, dtype=np.float64)
    beta = np.zeros(m_max - 1, dtype=np.float64)

    # If using Numba, we prefer Fortran order so that columns v[:, j] are contiguous in memory.
    # Benchmarks suggest a crossover around N=8000 where Numba becomes consistently faster (3x+).
    use_numba = HAS_NUMBA and vec.size >= 8192

    order = "C"
    v = np.zeros((vec.size, m_max), dtype=np.complex128, order=order)

    v0 = vec / vec_norm
    v[:, 0] = v0

    # Small constant for breakdown check
    eps_cut = 100.0 * vec.size * np.finfo(np.float64).eps

    # Cache for eigendecomposition to avoid recomputing if we reach max iterations
    cached_eigvals = None
    cached_eigvecs = None
    cached_k = None

    for j in range(m_max):
        vj = v[:, j]
        w = matrix_free_operator(vj)

        if use_numba:
            # Use JIT-compiled kernel for orthogonalization
            bj = orthogonalize_step(v, w, j, alpha, beta)

            if j < m_max - 1:
                if bj < eps_cut:
                    k = j + 1
                    return _compute_krylov_result(alpha[:k], beta[: k - 1], v[:, :k], float(vec_norm), dt)

                normalize_and_store(v, w, j, bj)
        else:
            # Pure Python implementation
            # alpha_j = <v_j, w>
            aj = np.vdot(vj, w).real
            alpha[j] = aj

            # Orthogonalize
            w -= aj * vj
            if j > 0:
                w -= beta[j - 1] * v[:, j - 1]

            # Beta_j
            if j < m_max - 1:
                bj = np.linalg.norm(w)
                beta[j] = bj

                if bj < eps_cut:
                    k = j + 1
                    return _compute_krylov_result(alpha[:k], beta[: k - 1], v[:, :k], float(vec_norm), dt)

                v[:, j + 1] = w / bj

        # Error check (adaptive)
        # We need to compute the exponential of the current T_j
        # Error approx: beta_j * | [exp(-i*dt*T_j)]_{j, 0} |  (last element of first column)
        # We only do this if j >= 1 (at least 2 vectors) to have a meaningful check, or even later.
        if j >= 1:
            # Current tridiagonal matrix T_k of size k x k where k = j + 1
            k = j + 1
            alpha_ = alpha[:k]
            beta_ = beta[: k - 1]  # beta has length k-1

            # Diagonalize T_k
            try:
                w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
                    alpha_,
                    beta_,
                    lapack_driver="stemr",
                    check_finite=False,
                )
            except scipy.linalg.LinAlgError:
                w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
                    alpha_,
                    beta_,
                    lapack_driver="stebz",
                    check_finite=False,
                )

            # Cache this eigendecomposition
            cached_eigvals = w_hess
            cached_eigvecs = u_hess
            cached_k = k

            coeffs = np.exp(-1j * dt * w_hess) * u_hess[0, :]
            phi_last = np.dot(u_hess[k - 1, :], coeffs)

            if j < m_max - 1:
                # Note: beta array is 0-indexed, so beta[j] corresponds to beta_{j} (the one we just computed)
                # Ensure we have computed it (we did above)
                err = beta[j] * abs(phi_last)
                if err < tol:
                    # Converged
                    y_k = u_hess @ coeffs
                    return np.asarray(v[:, :k] @ (y_k * vec_norm), dtype=np.complex128)

    # If we reach here, we exhausted max_lanczos_iterations
    # Check if we can reuse the cached eigendecomposition
    if cached_eigvals is not None and cached_eigvecs is not None and cached_k == m_max:
        # Reuse the cached eigendecomposition from the last error check
        coeffs = vec_norm * np.exp(-1j * dt * cached_eigvals) * cached_eigvecs[0]
        return np.asarray(v @ (cached_eigvecs @ coeffs), dtype=np.complex128)

    # Otherwise compute fresh (shouldn't happen often)
    return _compute_krylov_result(alpha, beta, v, float(vec_norm), dt)


def _compute_krylov_result(
    alpha: NDArray[np.float64],
    beta: NDArray[np.float64],
    lanczos_mat: NDArray[np.complex128],
    nrm: float,
    dt: float,
) -> NDArray[np.complex128]:
    """Compute the final approximation using the Krylov subspace.

    Constructs the tridiagonal matrix T_k from alpha and beta, computes its
    matrix exponential exp(-1j * dt * T_k), and projects the result back to the
    full space using the Lanczos basis vectors V_k.

    Args:
        alpha (NDArray[np.float64]): The diagonal elements of the tridiagonal matrix.
        beta (NDArray[np.float64]): The off-diagonal elements of the tridiagonal matrix.
        lanczos_mat (NDArray[np.complex128]): Matrix whose columns are the orthonormal Lanczos vectors.
        nrm (float): The Euclidean norm of the initial vector.
        dt (float): The time step or scaling factor for the exponential.

    Returns:
        NDArray[np.complex128]: The approximate result of the matrix exponential applied to the initial vector.
    """
    try:
        w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
            alpha,
            beta,
            lapack_driver="stemr",
            check_finite=False,
        )
    except scipy.linalg.LinAlgError:
        w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
            alpha,
            beta,
            lapack_driver="stebz",
            check_finite=False,
        )
    coeffs = nrm * np.exp(-1j * dt * w_hess) * u_hess[0]
    return np.asarray(lanczos_mat @ (u_hess @ coeffs), dtype=np.complex128)
