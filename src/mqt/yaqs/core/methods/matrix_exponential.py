# Copyright (c) 2025 - 2026 Chair for Design Automation, TUM
# All rights reserved.
#
# SPDX-License-Identifier: MIT
#
# Licensed under the MIT License

"""Fast Matrix Exponential Methods.

This module implements matrix-free methods for approximating the action of a matrix exponential
on a vector via Krylov subspace techniques. It provides an implementation of the Lanczos iteration
to generate an orthonormal basis for the Krylov subspace, and uses this basis to compute an
approximation of exp(-1j * dt * A) * v without explicitly constructing the matrix A.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import scipy.linalg

if TYPE_CHECKING:
    from collections.abc import Callable

    from numpy.typing import NDArray



def lanczos_iteration(
    matrix_free_operator: Callable[[NDArray[np.complex128]], NDArray[np.complex128]],
    vec: NDArray[np.complex128],
    lanczos_iterations: int,
) -> tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.complex128]]:
    """Perform a matrix-free Lanczos iteration.

    This function generates an orthonormal basis for the Krylov subspace of the operator defined
    by matrix_free_operator using the Lanczos algorithm. It computes the diagonal (alpha) and off-diagonal (beta)
    elements of the tridiagonal (Hessenberg) matrix associated with the iteration and returns the
    matrix whose columns are the Lanczos vectors.

    Args:
        matrix_free_operator (Callable[[NDArray[np.complex128]], NDArray[np.complex128]]):
            A function that applies a linear transformation to a vector without explicitly constructing
            the matrix (i.e., matrix-free).
        vec (NDArray[np.complex128]):
            The starting vector for the Lanczos iteration. This vector is normalized in place.
        lanczos_iterations (int):
            The number of Lanczos iterations to perform. It should be much smaller than the dimension of
            vec.

    Returns:
        tuple:
            - alpha (NDArray[np.float64]): Array of length lanczos_iterations containing the diagonal entries of the
              tridiagonal matrix.
            - beta (NDArray[np.float64]): Array of length lanczos_iterations-1 containing the off-diagonal entries.
            - lanczos_mat (NDArray[np.complex128]): A matrix of shape (len(vec) x lanczos_iterations) whose
              columns are the orthonormal Lanczos vectors.

    Raises:
        ValueError: If the starting vector has zero norm.
    """
    v0 = np.array(vec, dtype=np.complex128, copy=True)
    nrm = np.linalg.norm(v0)
    if nrm == 0:
        msg = "Starting vector has zero norm."
        raise ValueError(msg)
    v0 /= nrm

    m = lanczos_iterations
    alpha = np.zeros(m, dtype=np.float64)
    beta = np.zeros(m - 1, dtype=np.float64)

    # Store basis as (n, m): V[:, j] is contiguous and BLAS-friendly
    v = np.zeros((v0.size, m), dtype=np.complex128)
    v[:, 0] = v0

    eps_cut = 100.0 * v0.size * np.finfo(np.float64).eps

    for j in range(m - 1):
        vj = v[:, j]
        w = matrix_free_operator(vj)  # expect shape (n,)

        # alpha_j = <v_j, w>
        aj = np.vdot(vj, w).real
        alpha[j] = aj

        # w <- w - aj*vj - beta_{j-1}*v_{j-1}
        w -= aj * vj
        if j > 0:
            w -= beta[j - 1] * v[:, j - 1]

        bj = np.linalg.norm(w)
        beta[j] = bj

        if bj < eps_cut:
            # Early termination: return truncated basis
            k = j + 1
            return alpha[:k], beta[: k - 1], v[:, :k]

        v[:, j + 1] = w / bj

    # Final alpha_{m-1}
    vj = v[:, m - 1]
    w = matrix_free_operator(vj)
    alpha[m - 1] = np.vdot(vj, w).real
    return alpha, beta, v


def expm_krylov(
    matrix_free_operator: Callable[[NDArray[np.complex128]], NDArray[np.complex128]],
    vec: NDArray[np.complex128],
    dt: float,
    max_lanczos_iterations: int = 25,
    tol: float = 1e-12,
) -> NDArray[np.complex128]:
    """Compute the Krylov subspace approximation of the matrix exponential applied to a vector.

    This function approximates exp(-1j * dt * A) * v by projecting the action of the matrix exponential
    onto a Krylov subspace generated by the Lanczos iteration. The method is based on the approach
    described by Hochbruck and Lubich.

    Adaptive Step Size Control:
    The iteration proceeds until the estimated error is below the provided tolerance `tol` or
    `max_lanczos_iterations` is reached. The error is estimated using the residual of the
    Lanczos recurrence relation.

    Args:
        matrix_free_operator (Callable[[NDArray[np.complex128]], NDArray[np.complex128]]):
            A function implementing the matrix-free application of the linear operator A.
        vec (NDArray[np.complex128]):
            The input vector to which the matrix exponential is applied.
        dt (float):
            The time step (or scalar multiplier) in the exponential.
        max_lanczos_iterations (int):
            The maximum number of Lanczos iterations to perform. Default is 25.
        tol (float):
            The convergence tolerance. Default is 1e-12.

    Returns:
        NDArray[np.complex128]:
            The approximate result of applying exp(-1j * dt * A) to vec.
    """
    vec_norm = np.linalg.norm(vec)
    if vec_norm == 0:
        return vec

    # We need to reimplement Lanczos to support adaptive stopping.
    # We will build alpha, beta, and V incrementally.
    m_max = max_lanczos_iterations
    alpha = np.zeros(m_max, dtype=np.float64)
    beta = np.zeros(m_max - 1, dtype=np.float64)
    v = np.zeros((vec.size, m_max), dtype=np.complex128)

    v0 = vec / vec_norm
    v[:, 0] = v0

    # Small constant for breakdown check
    eps_cut = 100.0 * vec.size * np.finfo(np.float64).eps

    # Initial guess for the coefficients (just e1 scaled)
    # At m=1, T is 1x1 [alpha[0]], we want to check convergence after at least a few steps?
    # Usually we check every step.

    for j in range(m_max):
        vj = v[:, j]
        w = matrix_free_operator(vj)

        # alpha_j = <v_j, w>
        aj = np.vdot(vj, w).real
        alpha[j] = aj

        # Orthogonalize
        w -= aj * vj
        if j > 0:
            w -= beta[j - 1] * v[:, j - 1]

        # Beta_j
        if j < m_max - 1:
            bj = np.linalg.norm(w)
            beta[j] = bj

            if bj < eps_cut:
                # Invariant subspace found, exact solution possible
                # Truncate to generated subspace size k = j + 1
                k = j + 1
                _alpha = alpha[:k]
                _beta = beta[: k - 1]
                _v = v[:, :k]
                return _compute_krylov_result(_alpha, _beta, _v, vec_norm, dt)

            v[:, j + 1] = w / bj

        # Error check (adaptive)
        # We need to compute the exponential of the current T_j
        # Error approx: beta_j * | [exp(-i*dt*T_j)]_{j, 0} |  (last element of first column)
        # We only do this if j >= 1 (at least 2 vectors) to have a meaningful check, or even later.
        if j >= 1:
            # Current tridiagonal matrix T_k of size k x k where k = j + 1
            k = j + 1
            _alpha = alpha[:k]
            _beta = beta[: k - 1]  # beta has length k-1

            # Diagonalize T_k
            try:
                w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
                    _alpha,
                    _beta,
                    lapack_driver="stemr",
                    check_finite=False,
                )
            except scipy.linalg.LinAlgError:
                w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
                    _alpha,
                    _beta,
                    lapack_driver="stebz",
                    check_finite=False,
                )

            # E_k = U exp(-i dt D) U^H
            # We need the first column of E_k, say y_k. y_k = U @ (exp(...) * (U^H e1))
            # u_hess is (k, k). u_hess[0, :] is the first row of U, which is (U^H e1)^T (since U is real/orth).
            # So coeffs = exp(...) * u_hess[0, :]
            # element we want is the last element of y_k: [y_k]_{k-1}
            # y_k = u_hess @ coeffs
            # last_el = u_hess[k-1, :] @ coeffs

            coeffs = np.exp(-1j * dt * w_hess) * u_hess[0, :]
            phi_last = np.dot(u_hess[k - 1, :], coeffs)

            # Error estimate: beta_{j} * |phi_last| (using beta[j] which is beta_{k-1})
            # Note: beta array is 0-indexed, so beta[k-1] exists if we are not at the very last step.
            # However, we compute beta[j] BEFORE this check.
            # If j == m_max - 1, we don't have beta[j]. The loop finishes anyway.
            
            if j < m_max - 1:
                err = beta[j] * abs(phi_last)
                if err < tol:
                    # Converged
                    # Reconstruct result using the current subspace
                    # y = V_k @ y_k
                    y_k = u_hess @ coeffs
                    return np.asarray(v[:, :k] @ (y_k * vec_norm), dtype=np.complex128)

    # If we reach here, we exhausted max_lanczos_iterations
    # Using the full basis generated.
    return _compute_krylov_result(alpha, beta, v, vec_norm, dt)


def _compute_krylov_result(
    alpha: NDArray[np.float64],
    beta: NDArray[np.float64],
    lanczos_mat: NDArray[np.complex128],
    nrm: float,
    dt: float,
) -> NDArray[np.complex128]:
    """Helper to compute final result from T and V."""
    try:
        w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
            alpha,
            beta,
            lapack_driver="stemr",
            check_finite=False,
        )
    except scipy.linalg.LinAlgError:
        w_hess, u_hess = scipy.linalg.eigh_tridiagonal(
            alpha,
            beta,
            lapack_driver="stebz",
            check_finite=False,
        )
    coeffs = nrm * np.exp(-1j * dt * w_hess) * u_hess[0]
    return np.asarray(lanczos_mat @ (u_hess @ coeffs), dtype=np.complex128)

